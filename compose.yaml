
services:
  
  vllm:
    image: rocm/vllm:latest
    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    
    group_add:
      - "video"
      - "render"
    ipc: 
      "host"
    ports: 
      - "8001:8001"
    cap_add:
      - SYS_PTRACE
    environment: 
      HIP_VISIBLE_DEVICES: 2,3
      #ROCR_VISIBLE_DEVICES: 5,6

      # one of these makes it work and idk which ones
      # VLLM_SKIP_P2P_CHECK: 1
      # NCCL_CUMEM_ENABLE: 0
      # NCCL_IB_DISABLE: 1
      HSA_ENABLE_IPC_MODE_LEGACY: 0
      # why not have all of them 

      HF_HUB_CACHE: /hf_home

    volumes:
      - /shared/huggingface:/hf_home
      - /home/amysuo12/amd2025test/vllm:/app
    command: ["/bin/sh", 
              "-c", 
              "vllm serve Salesforce/Llama-xLAM-2-70b-fc-r --port 8001 --enable-auto-tool-choice --tool-parser-plugin ./xlam_tool_call_parser.py --tool-call-parser xlam --enforce-eager --gpu-memory-utilization 0.95 --tensor-parallel-size 2
              "]
      # --tensor-parallel-size 2
      # --max-model-len 74782
      # Salesforce/Llama-xLAM-2-70b-fc-r
      #         --enable-auto-tool-choice
      #         --tool-parser-plugin ./xlam_tool_call_parser.py
      #         --tool-call-parser xlam
  kokoro-fastapi:
    ports:
      - 8880:8880
    image: kokoro-fastapi-rocm
    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    ipc: "host"
    expose:
      - "3000"
    group_add:
      - "video"
      - "render"
      - "audio"
    cap_add:
      - SYS_PTRACE
    environment: 
      HIP_VISIBLE_DEVICES: 1
    restart: always
    command: ["python", "-m", "uvicorn", "api.src.main:app", "--host", "0.0.0.0", "--port", "8880", "--log-level", "debug"]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main

    ports:
      - "3000:8080"

    restart: always
    ipc: "host"
    
    volumes:
      - /home/amysuo12/amd2025test/open-webui:/app/backend/data
    devices:
      - /dev/kfd
      - /dev/dri    
    environment:
      OPENAI_API_BASE_URL: "http://vllm:8001/v1"
      # what open webui needs to connect to vllm automatically
      ENABLE_OLLAMA_API: false
      AUDIO_TTS_OPENAI_API_BASE_URL: http://localhost:8880/v1
  
  mcp:
    build:
      context: .
      dockerfile: Dockerfile
    ports: 
      - "8002:8002"
    depends_on: 
      - vllm
    environment:
      OPENAI_API_BASE_URL: "http://vllm:8001/v1"
      OPENAI_KEY: "EMPTY"
      OPENAI_MODEL: "Salesforce/Llama-xLAM-2-70b-fc-r"
    command: ["mcpo", "--host", "0.0.0.0", "--port", "8002", "--", "python", "poem.py"]



  # kokoro-fastapi-cpu:
  #   ports:
  #       - 8880:8880
  #   image: ghcr.io/remsky/kokoro-fastapi-cpu
  #   restart: always