
services:
  vllm:
  # model: /shared/huggingface/hub/models--amd--Llama-3.1-8B-Instruct-FP8-KV/
    image: vllm-toolkit:latest
    #context: ../../../


    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    
    group_add:
      - "video"
      - "render"
    ipc: 
      "host"
    ports: 
      - "8000:8000"

    cap_add:
      - SYS_PTRACE
    environment: 
      HIP_VISIBLE_DEVICES: 2
      MODE: serve
      MODEL: "llama2-70b-chat-hf/orig"
      PORT: 8000
      VLLM_LOGGING_LEVEL: DEBUG
      ARGS: "--enforce-eager"

    volumes:
      - /shared/inference/model/
  
  # open-webui:
  #   image: ghcr.io/open-webui/open-webui:main

  #   ports:
  #     - "3000:8000"

  #   volumes:
  #     - open-webui:/app/backend/data
  #   devices:
  #     - /dev/kfd
  #     - /dev/dri
  #   environment:
  #     OLLAMA_BASE_URL: http://localhost:11434/
      