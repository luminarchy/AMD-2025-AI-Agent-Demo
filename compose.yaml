
services:
  vllm:
  # model: /shared/huggingface/hub/models--amd--Llama-3.1-8B-Instruct-FP8-KV/
    image: rocm/vllm:latest
    #context: ../../../


    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    
    group_add:
      - "video"
      - "render"
    ipc: 
      "shareable"
    ports: 
      - "8002:8002"
    cap_add:
      - SYS_PTRACE
    environment: 
      HIP_VISIBLE_DEVICES: 2
      # MODE: serve
      # MODEL: "llama2-70b-chat-hf/orig"
      HOST: 8002
      PORT: 8002
      # ARGS: "--enforce-eager"
      HF_HUB_CACHE: /hf_home

    volumes:
      - /shared/huggingface:/hf_home
    command: ["/bin/sh", "-c", "vllm serve amd/Llama-3.3-70B-Instruct-FP8-KV"]
  # ["sh", "-c", "while true; do sleep 1000; done"]
    
  #vllm serve amd/Llama-3.2-1B-FP8-KV
  open-webui:
    image: ghcr.io/open-webui/open-webui:main

    ports:
      - "3000:8080"

    volumes:
      - /home/amysuo12/amd2025test/open-webui:/app/backend/data
    devices:
      - /dev/kfd
      - /dev/dri
    environment:
      HIP_VISIBLE_DEVICES: 2
      OPENAI_API_BASE_URL: http://vllm:8002/v1

