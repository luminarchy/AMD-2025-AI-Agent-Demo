
services:
  vllm:
  # model: /shared/huggingface/hub/models--amd--Llama-3.1-8B-Instruct-FP8-KV/
    image: rocm/vllm:latest
    #context: ../../../
    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    
    group_add:
      - "video"
      - "render"
    ipc: 
      "host"
    ports: 
      - "8001:8001"
    cap_add:
      - SYS_PTRACE
    environment: 
      HIP_VISIBLE_DEVICES: 1,2
      # ROCR_VISIBLE_DEVICES: 1,3
      # NCCL_DEBUG: INFO
      # NCCL_CUMEM_ENABLE: 0
      # NCCL_IB_DISABLE: 1
      # MODE: serve
      # MODEL: "llama2-70b-chat-hf/orig"
      HOST: 8001
      PORT: 8001
      # TENSOR_PARALLEL_SIZE: 16
      # GPU_MEMORY_UTILIZATION: .95
      # MAX_NUM_SEQS: 256
      # MAX_MODEL_LEN: 8192
      HF_HUB_CACHE: /hf_home

    volumes:
      - /shared/huggingface:/hf_home
    command: ["/bin/sh", 
              "-c", 
              "vllm serve Salesforce/Llama-xLAM-2-70b-fc-r 
              --enforce-eager 
              --gpu-memory-utilization 0.95 --max-model-len 74000"]
  # ["sh", "-c", "while true; do sleep 1000; done"]
    
  #vllm serve amd/Llama-3.2-1B-FP8-KV
  open-webui:
    image: ghcr.io/open-webui/open-webui:main

    ports:
      - "3000:8080"

    volumes:
      - /home/amysuo12/amd2025test/open-webui:/app/backend/data
    devices:
      - /dev/kfd
      - /dev/dri    
    environment:
      OPENAI_API_BASE_URL: http://vllm:8001/v1
      ENABLE_OLLAMA_API: false


