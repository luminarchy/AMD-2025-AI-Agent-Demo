
services:
  vllm:
  # model: /shared/huggingface/hub/models--amd--Llama-3.1-8B-Instruct-FP8-KV/
    image: rocm/vllm:latest
    #context: ../../../


    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    
    group_add:
      - "video"
      - "render"
    ipc: 
      "shareable"
    ports: 
      - "8000:8000"
    cap_add:
      - SYS_PTRACE
    environment: 
      HIP_VISIBLE_DEVICES: 1
      # MODE: serve
      # MODEL: "llama2-70b-chat-hf/orig"
      HOST: 8000
      PORT: 8000
      # ARGS: "--enforce-eager"
      HF_HUB_CACHE: /hf_home

    volumes:
      - /shared/huggingface:/hf_home
    command: ["/bin/sh", "-c", "vllm serve amd/Llama-3.3-70B-Instruct-FP8-KV"]
  # ["sh", "-c", "while true; do sleep 1000; done"]
    
  #vllm serve amd/Llama-3.2-1B-FP8-KV
  open-webui:
    image: ghcr.io/open-webui/open-webui:main

    ports:
      - "2000:8080"

    volumes:
      - /home/amysuo12/amd2025test/open-webui:/app/backend/data
    devices:
      - /dev/kfd
      - /dev/dri    
    environment:
      OPENAI_API_BASE_URL: http://vllm:8000/v1
      ENABLE_OLLAMA_API: false
    restart: 
      always  

